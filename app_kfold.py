import sys
import streamlit as st
import torch
import numpy as np
import os
import cv2
import pathlib
from PIL import Image
from pathlib import Path
import torchvision.ops as ops
import matplotlib.pyplot as plt
import json
import io

# Windows í™˜ê²½ì—ì„œ 'PosixPath' ëŒ€ì‹  'WindowsPath'ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
if os.name == 'nt':
    pathlib.PosixPath = pathlib.WindowsPath

# YOLOv5 ê²½ë¡œ ì¶”ê°€
YOLOV5_PATH = str("./")
sys.path.append(YOLOV5_PATH)

# YOLOv5ì—ì„œ í•„ìš”í•œ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from models.experimental import attempt_load
from models.common import DetectMultiBackend
from utils.general import non_max_suppression, scale_boxes
from utils.torch_utils import select_device
from utils.augmentations import letterbox

# âœ… ì‚¬ìš©í•  YOLOv5 ëª¨ë¸ 5ê°œ ì„¤ì • (K-Fold ëª¨ë¸)
MODEL_PATHS = [
    "./runs/train/fold_1/weights/best_fold1.pt",
    "./runs/train/fold_2/weights/best_fold2.pt",
    "./runs/train/fold_3/weights/best_fold3.pt",
    "./runs/train/fold_4/weights/best_fold4.pt",
    "./runs/train/fold_5/weights/best_fold5.pt",
]

# âœ… ê°œë³„ YOLOv5 ëª¨ë¸ ì¶”ê°€
SINGLE_MODEL_PATH = "./runs/train/exp/weights/best.pt"

# âœ… YOLOv5 ëª¨ë¸ 5ê°œ ë¡œë“œ (K-Fold)
@st.cache_resource
def load_models():
    return [attempt_load(path) for path in MODEL_PATHS]

# âœ… ê°œë³„ YOLOv5 ëª¨ë¸ ë¡œë“œ
@st.cache_resource
def load_single_model():
    device = select_device('cpu')
    return DetectMultiBackend(SINGLE_MODEL_PATH, device=device, dnn=False)

# âœ… UI ê°œì„ : ì œëª© ë° ëª¨ë¸ ì„ íƒ ì¸í„°í˜ì´ìŠ¤ í–¥ìƒ
st.markdown("<h1 style='text-align: center; font-size: 50px;'>ğŸ”O-ring ë¶ˆëŸ‰í™•ì¸</h1>", unsafe_allow_html=True)

# âœ… ì‚¬ì´ë“œë°” ì¶”ê°€
st.sidebar.markdown("## ğŸ” ëª¨ë¸ ì„ íƒ")

# ê¸°ë³¸ì ìœ¼ë¡œ K-Fold ì•™ìƒë¸”ì´ ì„ íƒë˜ë„ë¡ ì„¤ì •
if "selected_model" not in st.session_state:
    st.session_state["selected_model"] = "K-Fold ì•™ìƒë¸”"

# âœ… ëª¨ë¸ ì„ íƒ ë²„íŠ¼ (ì‚¬ì´ë“œë°”ë¡œ ì´ë™)
if st.sidebar.button("ğŸ¯ K-Fold ì•™ìƒë¸”", key="kfold"):
    st.session_state["selected_model"] = "K-Fold ì•™ìƒë¸”"
if st.sidebar.button("ğŸš€ ë‹¨ì¼ YOLOv5 ëª¨ë¸", key="single"):
    st.session_state["selected_model"] = "ë‹¨ì¼ YOLOv5 ëª¨ë¸"

# ì„¸ì…˜ ìœ ì§€
model_type = st.session_state["selected_model"]
st.markdown(f"<h3 style='text-align: center; color: green;'>âœ… ì„ íƒëœ ëª¨ë¸: {model_type}</h3>", unsafe_allow_html=True)

# ì„ íƒëœ ëª¨ë¸ ë¡œë“œ
model_type = st.session_state["selected_model"]
st.sidebar.markdown(f"**âœ… ì„ íƒëœ ëª¨ë¸:** `{model_type}`")
models = load_models() if model_type == "K-Fold ì•™ìƒë¸”" else [load_single_model()]

# âœ… YOLOv5 í´ë˜ìŠ¤ ì´ë¦„ ì„¤ì •
CLASS_NAMES = ["extruded", "crack", "cutting", "side_stamped"]

# âœ… ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_image(image, final_size=(500, 500), margin=20):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        contour = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(contour)
        height, width, _ = image.shape
        x_new = max(0, x - margin)
        y_new = max(0, y - margin)
        x_end = min(width, x + w + margin)
        y_end = min(height, y + h + margin)

        cropped = image[y_new:y_end, x_new:x_end]
        resized_image = cv2.resize(cropped, final_size, interpolation=cv2.INTER_LINEAR)
        return resized_image
    else:
        return cv2.resize(image, final_size, interpolation=cv2.INTER_LINEAR)

# âœ… YOLOv5 ì…ë ¥ ë³€í™˜ í•¨ìˆ˜
def prepare_image(image, img_size=640):
    img_resized, ratio, pad = letterbox(image, new_shape=img_size, auto=True)
    img_resized = img_resized[:, :, ::-1].transpose(2, 0, 1)  # HWC â†’ CHW
    img_resized = np.ascontiguousarray(img_resized)
    img_tensor = torch.from_numpy(img_resized).float() / 255.0
    img_tensor = img_tensor.unsqueeze(0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    return image, img_tensor, ratio, pad

# âœ… YOLOv5 ëª¨ë¸ ì˜ˆì¸¡ í•¨ìˆ˜
def get_predictions(models, img, conf_thres=0.3, iou_thres=0.5):
    all_boxes, all_scores, all_labels = [], [], []

    for model in models:
        with torch.no_grad():
            pred = model(img)[0]
        pred = non_max_suppression(pred, conf_thres, iou_thres)[0]

        if pred is not None and len(pred) > 0:
            all_boxes.append(pred[:, :4])
            all_scores.append(pred[:, 4])
            all_labels.append(pred[:, 5])

    if len(all_boxes) == 0:
        return [], [], []

    all_boxes = torch.cat(all_boxes, dim=0)
    all_scores = torch.cat(all_scores, dim=0)
    all_labels = torch.cat(all_labels, dim=0)

    return all_boxes, all_scores, all_labels

# âœ… NMS ì•™ìƒë¸” í•¨ìˆ˜
def ensemble_nms(boxes, scores, labels, iou_thres=0.5):
    keep_indices = ops.nms(boxes, scores, iou_thres)
    return boxes[keep_indices].cpu().numpy(), scores[keep_indices].cpu().numpy(), labels[keep_indices].cpu().numpy()

# âœ… ì‚¬ì´ë“œë°”ì— ì´ë¯¸ì§€ ì—…ë¡œë“œ ì¶”ê°€
st.sidebar.markdown("## ğŸ“‚ ì´ë¯¸ì§€ ì—…ë¡œë“œ")
uploaded_files = st.sidebar.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "png", "jpeg"], accept_multiple_files=True)

# ì„¸ì…˜ ìƒíƒœì—ì„œ í˜„ì¬ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ì €ì¥ (ì´ˆê¸°ê°’: 0)
if "image_index" not in st.session_state:
    st.session_state.image_index = 0

# âœ… has_defects ë³€ìˆ˜ë¥¼ ë¯¸ë¦¬ ì´ˆê¸°í™”
has_defects = False  # ê¸°ë³¸ê°’: ì´ë¯¸ì§€ê°€ ì—†ì„ ê²½ìš° ê²°í•¨ ì—†ìŒ

if uploaded_files:
    total_images = len(uploaded_files)

    if total_images > 1:
        selected_index = st.slider("ì´ë¯¸ì§€ ì„ íƒ", 0, total_images - 1, st.session_state.image_index)
        if selected_index != st.session_state.image_index:
            st.session_state.image_index = selected_index
            st.rerun()

    current_index = st.session_state.image_index
    uploaded_file = uploaded_files[current_index]
    image = Image.open(uploaded_file)
    image = np.array(image)

    # âœ… ì „ì²˜ë¦¬ ì ìš©
    processed_image = preprocess_image(image)

    # âœ… ëª¨ë¸ ì˜ˆì¸¡ê°’ ì–»ê¸°
    _, img_tensor, ratio, pad = prepare_image(processed_image)
    boxes, scores, labels = get_predictions(models, img_tensor)

    # âœ… ê²°í•¨ ì—¬ë¶€ ì²´í¬ (ì—¬ê¸°ì„œ has_defects ì—…ë°ì´íŠ¸)
    has_defects = len(boxes) > 0

    if has_defects:
        # âœ… NMS ì ìš© (ê²°í•¨ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ)
        nms_boxes, nms_scores, nms_labels = ensemble_nms(boxes, scores, labels)

        # âœ… ë°”ìš´ë”© ë°•ìŠ¤ ìœ„ì¹˜ ë³€í™˜ ì ìš©
        nms_boxes = scale_boxes(img_tensor.shape[2:], torch.tensor(nms_boxes), processed_image.shape).round().numpy()

        # âœ… ë°”ìš´ë”© ë°•ìŠ¤ ìƒ‰ìƒ ì§€ì • (í´ë˜ìŠ¤ë³„ ë‹¤ë¥¸ ìƒ‰ìƒ)
        COLORS = {"extruded": (0, 0, 255), "crack": (255, 0, 0), "cutting": (0, 255, 0), "side_stamped": (255, 255, 0)}

        # âœ… ê²°í•¨ ëª©ë¡ ì €ì¥
        defect_list = []

        # âœ… ê²°ê³¼ í‘œì‹œ
        for box, score, label in zip(nms_boxes, nms_scores, nms_labels):
            x1, y1, x2, y2 = map(int, box)
            class_name = CLASS_NAMES[int(label)] if int(label) < len(CLASS_NAMES) else f"Class {int(label)}"
            color = COLORS.get(class_name, (0, 255, 0))  # ê¸°ë³¸ ì´ˆë¡ìƒ‰

            # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            cv2.rectangle(processed_image, (x1, y1), (x2, y2), color, 3)

            # í…ìŠ¤íŠ¸ ë°°ê²½ ë°•ìŠ¤ ì¶”ê°€
            text = f"{class_name}: {score:.2f}"
            (w, h), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)
            cv2.rectangle(processed_image, (x1, y1 - h - 5), (x1 + w + 10, y1), color, -1)

            # í…ìŠ¤íŠ¸ ì¶”ê°€ (ë°°ê²½ ìœ„ì— í°ìƒ‰ ê¸€ì”¨)
            cv2.putText(processed_image, text, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

            # âœ… ê²°í•¨ ëª©ë¡ì— ì¶”ê°€
            defect_list.append(f"- **{class_name}** (ì‹ ë¢°ë„: {score:.2f})")

        # âœ… ê²°í•¨ ë©”ì‹œì§€ í‘œì‹œ
        st.markdown("<h3 style='text-align: center; color: red;'>âš ï¸ ê²°í•¨ì´ ê²€ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.</h3>", unsafe_allow_html=True)

        # âœ… ê²°í•¨ ëª©ë¡ ì¶œë ¥ (ì•„ë˜ì— í‘œì‹œ)
        for defect in defect_list:
            st.markdown(f"<h5 style='text-align: center; color: black;'>{defect}</h5>", unsafe_allow_html=True)

    else:
        st.markdown("<h3 style='text-align: center; color: green;'>âœ… ì •ìƒì…ë‹ˆë‹¤.</h3>", unsafe_allow_html=True)

    # âœ… JSON ë° ì´ë¯¸ì§€ ì €ì¥ ë²„íŠ¼ (ì‚¬ì´ë“œë°”ë¡œ ì´ë™)
    st.sidebar.markdown("## ğŸ“¥ ê²°ê³¼ ì €ì¥")

    json_data = {
        "image_name": uploaded_file.name,
        "detections": [
            {
                "class": CLASS_NAMES[int(label)] if int(label) < len(CLASS_NAMES) else f"Class {int(label)}",
                "confidence": round(float(score), 4),
                "bbox": list(map(int, box))
            }
            for box, score, label in zip(boxes, scores, labels)
        ]
    }

    json_bytes = io.BytesIO()
    json_bytes.write(json.dumps(json_data, indent=4).encode())
    json_bytes.seek(0)

    st.sidebar.download_button("ğŸ“¥ JSON ë‹¤ìš´ë¡œë“œ", data=json_bytes, file_name=f"{Path(uploaded_file.name).stem}.json", mime="application/json")

    pil_image = Image.fromarray(cv2.cvtColor(processed_image, cv2.COLOR_BGR2RGB))
    img_bytes = io.BytesIO()
    pil_image.save(img_bytes, format="PNG")
    img_bytes = img_bytes.getvalue()

    st.sidebar.download_button("ğŸ“¥ ê²°ê³¼ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ", data=img_bytes, file_name=f"result_{uploaded_file.name}", mime="image/png")

    # âœ… "ì´ì „" ë° "ë‹¤ìŒ" ë²„íŠ¼ì„ ëª¨ë“  ê²½ìš°ì—ì„œ í‘œì‹œ
    nav_container = st.container()  # ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±

    with nav_container:
        col1, col2 = st.columns([2, 2])

        with col1:
            if st.session_state.image_index > 0:
                if st.button("â¬…ï¸ ì´ì „ ì´ë¯¸ì§€", use_container_width=True):
                    st.session_state.image_index -= 1
                    st.rerun()

        with col2:
            if st.session_state.image_index < total_images - 1:
                if st.button("ë‹¤ìŒ ì´ë¯¸ì§€ â¡ï¸", use_container_width=True):
                    st.session_state.image_index += 1
                    st.rerun()
    
    # âœ… ì´ë¯¸ì§€ ì¶œë ¥ (ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ ì•„ë˜, ê²°í•¨ ìœ ë¬´ì™€ ê´€ê³„ì—†ì´ í‘œì‹œ)
    st.image(processed_image, caption=f"Detection Results - {uploaded_file.name}", use_container_width=True)