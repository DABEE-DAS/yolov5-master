import sys
import streamlit as st
import torch
import numpy as np
import os
os.environ['OPENCV_IO_ENABLE_OPENEXR'] = '0'
os.environ['QT_QPA_PLATFORM'] = 'offscreen'
import cv2
import pathlib
from PIL import Image
from pathlib import Path

# ğŸ”¹ Windows í™˜ê²½ì—ì„œ 'PosixPath' ëŒ€ì‹  'WindowsPath'ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •
if os.name == 'nt':
    pathlib.PosixPath = pathlib.WindowsPath

# YOLOv5 ê²½ë¡œ ì¶”ê°€
YOLOV5_PATH = str("./")
sys.path.append(YOLOV5_PATH)

# YOLOv5ì—ì„œ í•„ìš”í•œ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from models.common import DetectMultiBackend
from utils.general import non_max_suppression, scale_boxes
from utils.torch_utils import select_device
from utils.augmentations import letterbox  # YOLOv5 ë¦¬ì‚¬ì´ì§• ë°©ì‹ ì‚¬ìš©

# YOLOv5 ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜
@st.cache_resource(hash_funcs={DetectMultiBackend: lambda _: None})
def load_model():
    model_path = str("./runs/train/fold_2/weights/best.pt")
    device = select_device('cpu')
    model = DetectMultiBackend(model_path, device=device, dnn=False)
    return model

# ğŸ”¹ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ì™¸ê³½ì„  ê²€ì¶œ í›„ í¬ë¡­ & YOLOv5 ë°©ì‹ ë¦¬ì‚¬ì´ì¦ˆ)
def preprocess_image(image, final_size=(500, 500), margin=20):
    """
    1. ì´ë¯¸ì§€ë¥¼ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜ í›„ ì´ì§„í™”
    2. ì™¸ê³½ì„ ì„ ê²€ì¶œí•˜ì—¬ ê°€ì¥ í° ì˜ì—­ì„ ì°¾ìŒ
    3. í•´ë‹¹ ì˜ì—­ì„ í¬ë¡­ í›„ ì§€ì •ëœ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, binary = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)
    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        contour = max(contours, key=cv2.contourArea)
        x, y, w, h = cv2.boundingRect(contour)

        height, width, _ = image.shape
        x_new = max(0, x - margin)
        y_new = max(0, y - margin)
        x_end = min(width, x + w + margin)
        y_end = min(height, y + h + margin)

        cropped = image[y_new:y_end, x_new:x_end]

        # ë¦¬ì‚¬ì´ì¦ˆ (ë¹„ìœ¨ ìœ ì§€ ì—†ì´ ê°•ì œ ë³€í™˜)
        resized_image = cv2.resize(cropped, final_size, interpolation=cv2.INTER_LINEAR)
        return resized_image
    else:
        return cv2.resize(image, final_size, interpolation=cv2.INTER_LINEAR)

# Streamlit UI
st.title("Custom YOLOv5 Object Detection (Local Model)")
st.write("ë‚´ ì»´í“¨í„°ì˜ YOLOv5 ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê°ì²´ë¥¼ ê°ì§€í•©ë‹ˆë‹¤.")

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = load_model()

# YOLOv5 í´ë˜ìŠ¤ ë„¤ì„ ê°•ì œ ì„¤ì •
custom_names = ['extruded', 'crack', 'cutting', 'side stamped', 'object']
model.names = custom_names
names = model.names

st.write("Model Class Names:", names)

# ì´ë¯¸ì§€ ì—…ë¡œë“œ
uploaded_files = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "png", "jpeg"], accept_multiple_files=True)

# ì„¸ì…˜ ìƒíƒœì—ì„œ í˜„ì¬ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ì €ì¥ (ì´ˆê¸°ê°’: 0)
if "image_index" not in st.session_state:
    st.session_state.image_index = 0

if uploaded_files:
    # âœ… í•œ ê°œì˜ íŒŒì¼ë§Œ ì„ íƒë˜ì—ˆì„ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    if not isinstance(uploaded_files, list):
        uploaded_files = [uploaded_files]

    total_images = len(uploaded_files)

    # ğŸ¯ **ë“œë˜ê·¸ ë°”(Slider) ì¶”ê°€ (ì´ë¯¸ì§€ê°€ 1ê°œ ì´ìƒì¼ ë•Œë§Œ í‘œì‹œ)**
    if total_images > 1:
        selected_index = st.slider("ì´ë¯¸ì§€ ì„ íƒ", 0, total_images - 1, st.session_state.image_index)
        if selected_index != st.session_state.image_index:
            st.session_state.image_index = selected_index
            st.rerun()

    current_index = st.session_state.image_index
    uploaded_file = uploaded_files[current_index]
    image = Image.open(uploaded_file)
    image = np.array(image)

    # (â­ ì¶”ê°€) ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì ìš©
    processed_image = preprocess_image(image)

    # YOLOv5 ë¦¬ì‚¬ì´ì§• ë°©ì‹ ì ìš© (ë¹„ìœ¨ ìœ ì§€ + íŒ¨ë”©)
    img_resized, ratio, pad = letterbox(processed_image, new_shape=640, auto=True)

    # YOLOv5 ëª¨ë¸ ì…ë ¥ ì²˜ë¦¬
    img_resized = img_resized[:, :, ::-1].transpose(2, 0, 1)  # HWC â†’ CHW
    img_resized = np.ascontiguousarray(img_resized)

    img_tensor = torch.from_numpy(img_resized).to(model.device)
    img_tensor = img_tensor.float() / 255.0
    img_tensor = img_tensor.unsqueeze(0)

    with torch.no_grad():
        pred = model(img_tensor)
        pred = non_max_suppression(pred, 0.25, 0.45)

    detected_image = processed_image.copy()

    for det in pred:
        if det is not None and len(det):
            # âš ï¸ YOLOv5 ë°©ì‹ìœ¼ë¡œ ì›ë³¸ í¬ê¸°ì— ë§ê²Œ ë°”ìš´ë”© ë°•ìŠ¤ ë³€í™˜
            det[:, :4] = scale_boxes(img_tensor.shape[2:], det[:, :4], processed_image.shape).round()

            for *xyxy, conf, cls in det:
                x1, y1, x2, y2 = map(int, xyxy)
                class_id = int(cls)

                class_name = names[class_id] if class_id < len(names) else f"Class {class_id}"
                label = f"{class_name} {conf:.2f}"

                cv2.rectangle(detected_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)
                cv2.rectangle(detected_image, (x1, y1 - 20), (x1 + w, y1), (0, 255, 0), -1)
                cv2.putText(detected_image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)

    st.image(detected_image, caption=f"Detection Results - {uploaded_file.name}", use_column_width=True)
    st.write(f"ê°ì²´ ê°ì§€ ì™„ë£Œ - {uploaded_file.name}")

    # **"ì´ì „" ë° "ë‹¤ìŒ" ë²„íŠ¼ (ì´ë¯¸ì§€ê°€ 2ê°œ ì´ìƒì¼ ë•Œë§Œ í‘œì‹œ)**
    if total_images > 1:
        col1, col2 = st.columns([1, 1])

        with col1:
            if st.session_state.image_index > 0:
                if st.button("ì´ì „ ì´ë¯¸ì§€"):
                    st.session_state.image_index -= 1
                    st.rerun()

        with col2:
            if st.session_state.image_index < total_images - 1:
                if st.button("ë‹¤ìŒ ì´ë¯¸ì§€"):
                    st.session_state.image_index += 1
                    st.rerun()
